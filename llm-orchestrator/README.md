# LLM Orchestrator Component

This directory contains the query routing and AWS Bedrock integration logic.

## Status
ðŸš§ Not yet implemented - coming in Phase 2-3

## Documentation
See [docs/workflows/workflow_llm_orchestrator.md](../docs/workflows/workflow_llm_orchestrator.md) for development workflow.

## Structure (Planned)
```
llm-orchestrator/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ routers/
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ bedrock.py
â”‚   â”‚   â”œâ”€â”€ rag.py
â”‚   â”‚   â””â”€â”€ mcp.py
â”‚   â””â”€â”€ utils/
â””â”€â”€ tests/
```

## Tech Stack
- Python + FastAPI
- AWS SDK (boto3)
- Environment-driven configuration

## Responsibilities
- Receive queries from Chat UI
- Classify query type (RAG, MCP, or hybrid)
- Call AWS Bedrock API (Claude model)
- Invoke MCP tools
- Query Bedrock Knowledge Base
- Return unified responses with citations

## Next Steps
1. Set up FastAPI application skeleton
2. Implement Bedrock API client
3. Create query routing logic
4. Integrate with MCP server
5. Integrate with Bedrock Knowledge Base
